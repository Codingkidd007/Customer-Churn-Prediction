### Enterprise-Grade Customer Churn Prediction System

# Import necessary libraries
import os
import logging
import joblib
import pandas as pd
import numpy as np
import mlflow
import optuna
import shap
import lime.lime_tabular
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from kafka import KafkaProducer, KafkaConsumer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from pyspark.sql import SparkSession

# Initialize FastAPI app
app = FastAPI()

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Kafka setup
KAFKA_BROKER = "localhost:9092"
producer = KafkaProducer(bootstrap_servers=KAFKA_BROKER)
consumer = KafkaConsumer("churn_predictions", bootstrap_servers=KAFKA_BROKER)

# Spark session for big data processing
spark = SparkSession.builder.appName("CustomerChurnPrediction").getOrCreate()

# Load dataset
DATA_PATH = "customer_data.csv"
data = pd.read_csv(DATA_PATH)

# Data preprocessing
scaler = StandardScaler()
label_enc = LabelEncoder()
data["Churn"] = label_enc.fit_transform(data["Churn"])
data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    data.iloc[:, :-1], data["Churn"], test_size=0.2, random_state=42
)

# Hyperparameter tuning with Optuna
def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 500),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.3),
    }
    model = XGBClassifier(**params)
    model.fit(X_train, y_train)
    return model.score(X_test, y_test)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)
best_params = study.best_params

# Train final model
model = XGBClassifier(**best_params)
model.fit(X_train, y_train)

# Save model
joblib.dump(model, "churn_model.pkl")
logger.info("Model trained and saved successfully.")

# Define API request model
class ChurnInput(BaseModel):
    features: list

# Define prediction endpoint
@app.post("/predict")
def predict_churn(data: ChurnInput):
    try:
        input_data = np.array(data.features).reshape(1, -1)
        prediction = model.predict(input_data)[0]
        probability = model.predict_proba(input_data)[0][1]
        producer.send("churn_predictions", value=prediction)
        return {"churn": bool(prediction), "probability": probability}
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail="Error in prediction")

# Model explainability
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# LIME explanation
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train.values, feature_names=list(data.columns[:-1]), class_names=["No Churn", "Churn"], discretize_continuous=True
)

logger.info("Customer Churn Prediction API is up and running.")
